2025/03/31 18:09:38 routes.go:1215: INFO server config env="map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL:0 HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY:cuda OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/rmorain2/.ollama_models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:0 http_proxy: https_proxy: no_proxy:]"
time=2025-03-31T18:09:38.121-06:00 level=INFO source=images.go:432 msg="total blobs: 11"
time=2025-03-31T18:09:38.133-06:00 level=INFO source=images.go:439 msg="total unused blobs removed: 0"
time=2025-03-31T18:09:38.142-06:00 level=INFO source=routes.go:1277 msg="Listening on 127.0.0.1:11434 (version 0.5.13)"
time=2025-03-31T18:09:38.142-06:00 level=DEBUG source=sched.go:106 msg="starting llm scheduler"
time=2025-03-31T18:09:38.145-06:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-03-31T18:09:38.161-06:00 level=DEBUG source=gpu.go:98 msg="searching for GPU discovery libraries for NVIDIA"
time=2025-03-31T18:09:38.161-06:00 level=DEBUG source=gpu.go:501 msg="Searching for GPU library" name=libcuda.so*
time=2025-03-31T18:09:38.161-06:00 level=DEBUG source=gpu.go:525 msg="gpu library search" globs="[/home/rmorain2/.local/ollama/lib/ollama/libcuda.so* /home/rmorain2/git/LLMEXICA/libcuda.so* /usr/local/cuda*/targets/*/lib/libcuda.so* /usr/lib/*-linux-gnu/nvidia/current/libcuda.so* /usr/lib/*-linux-gnu/libcuda.so* /usr/lib/wsl/lib/libcuda.so* /usr/lib/wsl/drivers/*/libcuda.so* /opt/cuda/lib*/libcuda.so* /usr/local/cuda/lib*/libcuda.so* /usr/lib*/libcuda.so* /usr/local/lib*/libcuda.so*]"
time=2025-03-31T18:09:38.266-06:00 level=DEBUG source=gpu.go:558 msg="discovered GPU libraries" paths=[/usr/lib64/libcuda.so.570.86.15]
initializing /usr/lib64/libcuda.so.570.86.15
dlsym: cuInit - 0x7efda7e76e00
dlsym: cuDriverGetVersion - 0x7efda7e76e20
dlsym: cuDeviceGetCount - 0x7efda7e76e60
dlsym: cuDeviceGet - 0x7efda7e76e40
dlsym: cuDeviceGetAttribute - 0x7efda7e76f40
dlsym: cuDeviceGetUuid - 0x7efda7e76ea0
dlsym: cuDeviceGetName - 0x7efda7e76e80
dlsym: cuCtxCreate_v3 - 0x7efda7e77120
dlsym: cuMemGetInfo_v2 - 0x7efda7e778a0
dlsym: cuCtxDestroy - 0x7efda7ed59f0
calling cuInit
calling cuDriverGetVersion
raw version 0x2f30
CUDA driver version: 12.8
calling cuDeviceGetCount
device count 1
time=2025-03-31T18:09:38.401-06:00 level=DEBUG source=gpu.go:125 msg="detected GPUs" count=1 library=/usr/lib64/libcuda.so.570.86.15
[GPU-1cec1518-c78a-e0ed-fc6f-8ee96dc610d1] CUDA totalMem 81153 mb
[GPU-1cec1518-c78a-e0ed-fc6f-8ee96dc610d1] CUDA freeMem 80730 mb
[GPU-1cec1518-c78a-e0ed-fc6f-8ee96dc610d1] Compute Capability 8.0
time=2025-03-31T18:09:38.655-06:00 level=DEBUG source=amd_linux.go:419 msg="amdgpu driver not detected /sys/module/amdgpu"
releasing cuda driver library
time=2025-03-31T18:09:38.655-06:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-1cec1518-c78a-e0ed-fc6f-8ee96dc610d1 library=cuda variant=v12 compute=8.0 driver=12.8 name="NVIDIA A100-SXM4-80GB" total="79.3 GiB" available="78.8 GiB"
[GIN] 2025/03/31 - 18:09:47 | 200 |    4.766797ms |       127.0.0.1 | HEAD     "/"
[GIN] 2025/03/31 - 18:09:47 | 200 |  128.949947ms |       127.0.0.1 | POST     "/api/show"
time=2025-03-31T18:09:47.700-06:00 level=DEBUG source=gpu.go:391 msg="updating system memory data" before.total="1007.5 GiB" before.free="860.7 GiB" before.free_swap="7.8 GiB" now.total="1007.5 GiB" now.free="860.6 GiB" now.free_swap="7.8 GiB"
initializing /usr/lib64/libcuda.so.570.86.15
dlsym: cuInit - 0x7efda7e76e00
dlsym: cuDriverGetVersion - 0x7efda7e76e20
dlsym: cuDeviceGetCount - 0x7efda7e76e60
dlsym: cuDeviceGet - 0x7efda7e76e40
dlsym: cuDeviceGetAttribute - 0x7efda7e76f40
dlsym: cuDeviceGetUuid - 0x7efda7e76ea0
dlsym: cuDeviceGetName - 0x7efda7e76e80
dlsym: cuCtxCreate_v3 - 0x7efda7e77120
dlsym: cuMemGetInfo_v2 - 0x7efda7e778a0
dlsym: cuCtxDestroy - 0x7efda7ed59f0
calling cuInit
calling cuDriverGetVersion
raw version 0x2f30
CUDA driver version: 12.8
calling cuDeviceGetCount
device count 1
[?2026h[?25l[1G⠙ [K[?25h[?2026l[?2026h[?25l[1G⠹ [K[?25h[?2026l[?2026h[?25l[1G⠹ [K[?25h[?2026ltime=2025-03-31T18:09:47.946-06:00 level=DEBUG source=gpu.go:441 msg="updating cuda memory data" gpu=GPU-1cec1518-c78a-e0ed-fc6f-8ee96dc610d1 name="NVIDIA A100-SXM4-80GB" overhead="0 B" before.total="79.3 GiB" before.free="78.8 GiB" now.total="79.3 GiB" now.free="78.8 GiB" now.used="423.0 MiB"
releasing cuda driver library
time=2025-03-31T18:09:47.946-06:00 level=DEBUG source=sched.go:182 msg="updating default concurrency" OLLAMA_MAX_LOADED_MODELS=3 gpu_count=1
time=2025-03-31T18:09:47.988-06:00 level=DEBUG source=sched.go:225 msg="loading first model" model=/home/rmorain2/.ollama_models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc
time=2025-03-31T18:09:47.988-06:00 level=DEBUG source=memory.go:108 msg=evaluating library=cuda gpu_count=1 available="[78.8 GiB]"
time=2025-03-31T18:09:47.988-06:00 level=WARN source=ggml.go:136 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-03-31T18:09:47.988-06:00 level=WARN source=ggml.go:136 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-03-31T18:09:47.988-06:00 level=INFO source=sched.go:715 msg="new model will fit in available VRAM in single GPU, loading" model=/home/rmorain2/.ollama_models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc gpu=GPU-1cec1518-c78a-e0ed-fc6f-8ee96dc610d1 parallel=4 available=84652326912 required="1.9 GiB"
time=2025-03-31T18:09:47.989-06:00 level=DEBUG source=gpu.go:391 msg="updating system memory data" before.total="1007.5 GiB" before.free="860.6 GiB" before.free_swap="7.8 GiB" now.total="1007.5 GiB" now.free="860.6 GiB" now.free_swap="7.8 GiB"
initializing /usr/lib64/libcuda.so.570.86.15
dlsym: cuInit - 0x7efda7e76e00
dlsym: cuDriverGetVersion - 0x7efda7e76e20
dlsym: cuDeviceGetCount - 0x7efda7e76e60
dlsym: cuDeviceGet - 0x7efda7e76e40
dlsym: cuDeviceGetAttribute - 0x7efda7e76f40
dlsym: cuDeviceGetUuid - 0x7efda7e76ea0
dlsym: cuDeviceGetName - 0x7efda7e76e80
dlsym: cuCtxCreate_v3 - 0x7efda7e77120
dlsym: cuMemGetInfo_v2 - 0x7efda7e778a0
dlsym: cuCtxDestroy - 0x7efda7ed59f0
calling cuInit
calling cuDriverGetVersion
raw version 0x2f30
CUDA driver version: 12.8
calling cuDeviceGetCount
device count 1
[?2026h[?25l[1G⠼ [K[?25h[?2026l[?2026h[?25l[1G⠴ [K[?25h[?2026ltime=2025-03-31T18:09:48.236-06:00 level=DEBUG source=gpu.go:441 msg="updating cuda memory data" gpu=GPU-1cec1518-c78a-e0ed-fc6f-8ee96dc610d1 name="NVIDIA A100-SXM4-80GB" overhead="0 B" before.total="79.3 GiB" before.free="78.8 GiB" now.total="79.3 GiB" now.free="78.8 GiB" now.used="423.0 MiB"
releasing cuda driver library
time=2025-03-31T18:09:48.236-06:00 level=INFO source=server.go:97 msg="system memory" total="1007.5 GiB" free="860.6 GiB" free_swap="7.8 GiB"
time=2025-03-31T18:09:48.236-06:00 level=DEBUG source=memory.go:108 msg=evaluating library=cuda gpu_count=1 available="[78.8 GiB]"
time=2025-03-31T18:09:48.236-06:00 level=WARN source=ggml.go:136 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-03-31T18:09:48.236-06:00 level=WARN source=ggml.go:136 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-03-31T18:09:48.236-06:00 level=INFO source=server.go:130 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[78.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.9 GiB" memory.required.partial="1.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[1.9 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
time=2025-03-31T18:09:48.237-06:00 level=DEBUG source=server.go:259 msg="compatible gpu libraries" compatible="[cuda_v12 cuda_v11]"
time=2025-03-31T18:09:48.238-06:00 level=DEBUG source=server.go:302 msg="adding gpu library" path=/home/rmorain2/.local/ollama/lib/ollama/cuda_v12
time=2025-03-31T18:09:48.238-06:00 level=DEBUG source=server.go:310 msg="adding gpu dependency paths" paths=[/home/rmorain2/.local/ollama/lib/ollama/cuda_v12]
time=2025-03-31T18:09:48.238-06:00 level=INFO source=server.go:380 msg="starting llama server" cmd="/home/rmorain2/.local/ollama/bin/ollama runner --model /home/rmorain2/.ollama_models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --verbose --threads 128 --parallel 4 --port 42933"
time=2025-03-31T18:09:48.238-06:00 level=DEBUG source=server.go:398 msg=subprocess environment="[ROCR_VISIBLE_DEVICES=0 CUDA_VISIBLE_DEVICES=GPU-1cec1518-c78a-e0ed-fc6f-8ee96dc610d1 GPU_DEVICE_ORDINAL=0 PATH=/home/rmorain2/.local/ollama/bin:/home/rmorain2/.conda/envs/llmexica/bin:/apps/miniconda3/latest/condabin:/home/rmorain2/local/bin:/apps/slurm/latest/bin:/home/rmorain2/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/bin/remote-cli:/apps/slurm/latest/bin:/apps/lmod/lmod/libexec:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/rmorain2/bin:/opt/dell/srvadmin/bin:/home/rmorain2/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand:/home/rmorain2/.vscode-server/extensions/ms-python.debugpy-2025.4.1-linux-x64/bundled/scripts/noConfigScripts LD_LIBRARY_PATH=/home/rmorain2/.local/ollama/lib/ollama/cuda_v12:/home/rmorain2/.local/ollama/lib/ollama/cuda_v12:/home/rmorain2/.local/ollama/lib/ollama]"
[?2026h[?25l[1G⠴ [K[?25h[?2026ltime=2025-03-31T18:09:48.243-06:00 level=INFO source=sched.go:450 msg="loaded runners" count=1
time=2025-03-31T18:09:48.243-06:00 level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-03-31T18:09:48.244-06:00 level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-03-31T18:09:48.293-06:00 level=INFO source=runner.go:931 msg="starting go runner"
time=2025-03-31T18:09:48.294-06:00 level=DEBUG source=ggml.go:84 msg="ggml backend load all from path" path=/home/rmorain2/.local/ollama/lib/ollama/cuda_v12
[?2026h[?25l[1G⠧ [K[?25h[?2026l[?2026h[?25l[1G⠇ [K[?25h[?2026l[?2026h[?25l[1G⠏ [K[?25h[?2026l[?2026h[?25l[1G⠋ [K[?25h[?2026l[?2026h[?25l[1G⠙ [K[?25h[?2026l[?2026h[?25l[1G⠹ [K[?25h[?2026l[?2026h[?25l[1G⠹ [K[?25h[?2026l[?2026h[?25l[1G⠸ [K[?25h[?2026lggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /home/rmorain2/.local/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-03-31T18:09:49.085-06:00 level=DEBUG source=ggml.go:84 msg="ggml backend load all from path" path=/home/rmorain2/.local/ollama/lib/ollama
ggml_backend_load_best: /home/rmorain2/.local/ollama/lib/ollama/libggml-cpu-alderlake.so score: 0
ggml_backend_load_best: /home/rmorain2/.local/ollama/lib/ollama/libggml-cpu-haswell.so score: 55
ggml_backend_load_best: /home/rmorain2/.local/ollama/lib/ollama/libggml-cpu-icelake.so score: 0
ggml_backend_load_best: /home/rmorain2/.local/ollama/lib/ollama/libggml-cpu-sandybridge.so score: 20
[?2026h[?25l[1G⠴ [K[?25h[?2026lggml_backend_load_best: /home/rmorain2/.local/ollama/lib/ollama/libggml-cpu-skylakex.so score: 0
load_backend: loaded CPU backend from /home/rmorain2/.local/ollama/lib/ollama/libggml-cpu-haswell.so
time=2025-03-31T18:09:49.170-06:00 level=INFO source=runner.go:934 msg=system info="CPU : LLAMAFILE = 1 | CUDA : ARCHS = 500,600,610,700,750,800,860,870,890,900,1200 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | cgo(gcc)" threads=128
time=2025-03-31T18:09:49.170-06:00 level=INFO source=runner.go:992 msg="Server listening on 127.0.0.1:42933"
[?2026h[?25l[1G⠦ [K[?25h[?2026ltime=2025-03-31T18:09:49.248-06:00 level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
[?2026h[?25l[1G⠧ [K[?25h[?2026lllama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100-SXM4-80GB) - 80730 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/rmorain2/.ollama_models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[?2026h[?25l[1G⠇ [K[?25h[?2026lllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.04 GiB (5.00 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 151660 '<|fim_middle|>' is not marked as EOG
load: control token: 151659 '<|fim_prefix|>' is not marked as EOG
load: control token: 151653 '<|vision_end|>' is not marked as EOG
load: control token: 151645 '<｜Assistant｜>' is not marked as EOG
load: control token: 151644 '<｜User｜>' is not marked as EOG
load: control token: 151655 '<|image_pad|>' is not marked as EOG
load: control token: 151651 '<|quad_end|>' is not marked as EOG
load: control token: 151646 '<｜begin▁of▁sentence｜>' is not marked as EOG
load: control token: 151643 '<｜end▁of▁sentence｜>' is not marked as EOG
load: control token: 151652 '<|vision_start|>' is not marked as EOG
load: control token: 151647 '<|EOT|>' is not marked as EOG
load: control token: 151654 '<|vision_pad|>' is not marked as EOG
load: control token: 151656 '<|video_pad|>' is not marked as EOG
load: control token: 151661 '<|fim_suffix|>' is not marked as EOG
load: control token: 151650 '<|quad_start|>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
[?2026h[?25l[1G⠏ [K[?25h[?2026lload: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 1536
print_info: n_layer          = 28
print_info: n_head           = 12
print_info: n_head_kv        = 2
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 6
print_info: n_embd_k_gqa     = 256
print_info: n_embd_v_gqa     = 256
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8960
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.5B
print_info: model params     = 1.78 B
print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CUDA0
load_tensors: layer   1 assigned to device CUDA0
load_tensors: layer   2 assigned to device CUDA0
load_tensors: layer   3 assigned to device CUDA0
load_tensors: layer   4 assigned to device CUDA0
load_tensors: layer   5 assigned to device CUDA0
load_tensors: layer   6 assigned to device CUDA0
load_tensors: layer   7 assigned to device CUDA0
load_tensors: layer   8 assigned to device CUDA0
load_tensors: layer   9 assigned to device CUDA0
load_tensors: layer  10 assigned to device CUDA0
load_tensors: layer  11 assigned to device CUDA0
load_tensors: layer  12 assigned to device CUDA0
load_tensors: layer  13 assigned to device CUDA0
load_tensors: layer  14 assigned to device CUDA0
load_tensors: layer  15 assigned to device CUDA0
load_tensors: layer  16 assigned to device CUDA0
load_tensors: layer  17 assigned to device CUDA0
load_tensors: layer  18 assigned to device CUDA0
load_tensors: layer  19 assigned to device CUDA0
load_tensors: layer  20 assigned to device CUDA0
load_tensors: layer  21 assigned to device CUDA0
load_tensors: layer  22 assigned to device CUDA0
load_tensors: layer  23 assigned to device CUDA0
load_tensors: layer  24 assigned to device CUDA0
load_tensors: layer  25 assigned to device CUDA0
load_tensors: layer  26 assigned to device CUDA0
load_tensors: layer  27 assigned to device CUDA0
load_tensors: layer  28 assigned to device CUDA0
load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead
[?2026h[?25l[1G⠋ [K[?25h[?2026l[?2026h[?25l[1G⠙ [K[?25h[?2026l[?2026h[?25l[1G⠹ [K[?25h[?2026l[?2026h[?25l[1G⠹ [K[?25h[?2026l[?2026h[?25l[1G⠸ [K[?25h[?2026l[?2026h[?25l[1G⠴ [K[?25h[?2026l[?2026h[?25l[1G⠦ [K[?25h[?2026l[?2026h[?25l[1G⠧ [K[?25h[?2026l[?2026h[?25l[1G⠇ [K[?25h[?2026l[?2026h[?25l[1G⠏ [K[?25h[?2026l[?2026h[?25l[1G⠋ [K[?25h[?2026l[?2026h[?25l[1G⠋ [K[?25h[?2026l[?2026h[?25l[1G⠙ [K[?25h[?2026l[?2026h[?25l[1G⠹ [K[?25h[?2026l[?2026h[?25l[1G⠼ [K[?25h[?2026l[?2026h[?25l[1G⠴ [K[?25h[?2026l[?2026h[?25l[1G⠴ [K[?25h[?2026l[?2026h[?25l[1G⠧ [K[?25h[?2026l[?2026h[?25l[1G⠇ [K[?25h[?2026lload_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =   934.70 MiB
load_tensors:   CPU_Mapped model buffer size =   125.19 MiB
[?2026h[?25l[1G⠇ [K[?25h[?2026l[?2026h[?25l[1G⠋ [K[?25h[?2026lllama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256
llama_kv_cache_init:      CUDA0 KV buffer size =   224.00 MiB
llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.34 MiB
llama_init_from_model:      CUDA0 compute buffer size =   299.75 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    19.01 MiB
llama_init_from_model: graph nodes  = 986
llama_init_from_model: graph splits = 2
[?2026h[?25l[1G⠋ [K[?25h[?2026ltime=2025-03-31T18:09:51.755-06:00 level=INFO source=server.go:596 msg="llama runner started in 3.51 seconds"
time=2025-03-31T18:09:51.755-06:00 level=DEBUG source=sched.go:463 msg="finished setting up runner" model=/home/rmorain2/.ollama_models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc
[GIN] 2025/03/31 - 18:09:51 | 200 |  4.116923193s |       127.0.0.1 | POST     "/api/generate"
time=2025-03-31T18:09:51.755-06:00 level=DEBUG source=sched.go:467 msg="context for request finished"
time=2025-03-31T18:09:51.755-06:00 level=DEBUG source=sched.go:340 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/home/rmorain2/.ollama_models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc duration=5m0s
time=2025-03-31T18:09:51.755-06:00 level=DEBUG source=sched.go:358 msg="after processing request finished event" modelPath=/home/rmorain2/.ollama_models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc refCount=0
[?25l[?2026h[?25l[1G[K[?25h[?2026l[2K[1G[?25h[?25l[?25htime=2025-03-31T18:09:54.404-06:00 level=DEBUG source=sched.go:576 msg="evaluating already loaded" model=/home/rmorain2/.ollama_models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc
time=2025-03-31T18:09:54.404-06:00 level=DEBUG source=routes.go:1501 msg="chat request" images=0 prompt="<｜User｜>Does the sun rise in the west? Just answer yes or no.<｜Assistant｜>"
time=2025-03-31T18:09:54.406-06:00 level=DEBUG source=cache.go:104 msg="loading cache slot" id=0 cache=0 prompt=17 used=0 remaining=17
[GIN] 2025/03/31 - 18:09:55 | 200 |  945.219151ms |       127.0.0.1 | POST     "/api/chat"
time=2025-03-31T18:09:55.309-06:00 level=DEBUG source=sched.go:408 msg="context for request finished"
time=2025-03-31T18:09:55.309-06:00 level=DEBUG source=sched.go:340 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/home/rmorain2/.ollama_models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc duration=5m0s
time=2025-03-31T18:09:55.309-06:00 level=DEBUG source=sched.go:358 msg="after processing request finished event" modelPath=/home/rmorain2/.ollama_models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc refCount=0
OLLAMA MODEL:  deepseek-r1:1.5b
<think>

</think>

Yes, the sun does rise from the west.
Reading story from file:  stories/jaguar_knight.txt
Story:  JAGUAR_KNIGHT WAS AN INHABITANT OF THE GREAT TENOCHTITLAN. PRINCESS WAS AN INHABITANT OF THE GREAT TENOCHTITLAN. TLALOC —THE GOD OF THE RAIN— WAS ANGRY AND SENT A STORM. THE HEAVY RAIN DAMAGED THE OLD WOODEN BRIDGE. WHEN JAGUAR_KNIGHT TRIED TO CROSS THE RIVER THE BRIDGE COLLAPSED INJURING BADLY JAGUAR_KNIGHT'S HEAD. PRINCESS KNEW THAT JAGUAR_KNIGHT COULD DIE AND THAT PRINCESS HAD TO DO SOMETHING ABOUT IT. PRINCESS HAD HEARD THAT THE TEPESCOHUITLE WAS AN EFFECTIVE CURATIVE PLANT. SO, PRINCESS PREPARED A PLASMA AND APPLIED IT TO JAGUAR_KNIGHT'S WOUNDS. IT WORKED AND JAGUAR_KNIGHT STARTED TO RECUPERATE! JAGUAR_KNIGHT REALISED THAT PRINCESS'S DETERMINATION HAD SAVED JAGUAR_KNIGHT'S LIFE. DURING THE LAST WAR PRINCESS'S FATHER HUMILIATED ENEMY'S FAMILY. NOW, IT WAS TIME OF REVENGE AND ENEMY KIDNAPPED PRINCESS. THEY WENT TO THE FOREST WHERE ENEMY TIED PRINCESS TO A HUGE ROCK. EXACTLY AT MIDNIGHT ENEMY WOULD CUT PRINCESS UP. ALTHOUGH IT WAS VERY DANGEROUS JAGUAR_KNIGHT DECIDED TO DO SOMETHING IN ORDER TO LIBERATE PRINCESS. FOR SOME MINUTES JAGUAR_KNIGHT PRAYED TO QUETZALCOATL —THE FEATHERED SNAKE, THE GOD BETWEEN THE GODS— AND ASKED FOR WISDOM AND BRAVENESS. NOW JAGUAR_KNIGHT WAS READY TO FIND OUT ITS FATE. PRINCESS WAS REALLY ANGRY FOR WHAT HAD HAPPENED AND AFFRONTED ENEMY. ENEMY'S FRAME OF MIND WAS VERY VOLATILE AND WITHOUT THINKING ABOUT IT ENEMY CHARGED AGAINST PRINCESS. MEANWHILE JAGUAR_KNIGHT DECIDED TO START A SEARCH FOR ENEMY. AFTER HARD WORK AND DIFFICULT MOMENTS JAGUAR_KNIGHT COULD FINALLY FIND ENEMY. JAGUAR_KNIGHT, FULL OF ANGER, TOOK A DAGGER AND ATTACKED ENEMY. JAGUAR_KNIGHT THREW SOME DUST IN ENEMY'S FACE. THEN, USING A DAGGER JAGUAR_KNIGHT PERFORATED ENEMY'S CHEST. IMITATING THE SACRED CEREMONY OF THE SACRIFICE, JAGUAR_KNIGHT TOOK ENEMY'S HEART WITH ONE HAND AND RAISED IT TOWARDS THE SUN AS A SIGN OF RESPECT TO THE GODS. JAGUAR_KNIGHT WALKED TOWARDS PRINCESS. FULL OF ADMIRATION FOR ALL THE BRAVENESS THAT PRINCESS HAD SHOWN IN THOSE HARD MOMENTS JAGUAR_KNIGHT LIBERATED PRINCESS! ALTHOUGH AT THE BEGINNING PRINCESS DID NOT WANT TO ADMIT IT, PRINCESS FELL IN LOVE WITH JAGUAR_KNIGHT. PRINCESS WAS KISSING JAGUAR_KNIGHT WHEN SUDDENLY PRINCESS RECOGNISED JAGUAR_KNIGHT'S TATTOO. IT WAS THE SAME AS THE ONE USED BY THE FRATERNITY WHICH HAD MURDERED PRINCESS'S FATHER SOME MONTHS AGO. AT ONCE ALL THOSE TERRIBLE MEMORIES WERE PRESENT AGAIN. PRINCESS HAD AMBIVALENT THOUGHTS TOWARDS JAGUAR_KNIGHT. ON THE ONE HAND PRINCESS HAD STRONG FEELINGS FOR JAGUAR_KNIGHT BUT ON THE OTHER HAND PRINCESS ABOMINATED WHAT JAGUAR_KNIGHT DID. PRINCESS FELT A DEEPLY ODIUM FOR JAGUAR_KNIGHT. INVOKING HUITZILOPOCHTLI, GOD OF THE DEAD, PRINCESS CUT JAGUAR_KNIGHT'S JUGULAR. THE BLOOD COVERED THE FLOOR. PRINCESS TOOK A DAGGER AND CUT PRINCESS’S THROAT. PRINCESS BLED TO DEATH WHILE TONATIU (THE GOD REPRESENTING THE SUN) DISAPPEARED IN THE HORIZON.
Traceback (most recent call last):
  File "/home/rmorain2/git/LLMEXICA/parse_story.py", line 256, in <module>
    main()
  File "/home/rmorain2/git/LLMEXICA/parse_story.py", line 81, in main
    with open("prompts/story_action.txt", "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'prompts/story_action.txt'
